{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limestone Data Challenge\n",
    "### Contributers: Pratik Sahoo, Satyankar Chandra\n",
    "\n",
    "Hi and welcome to our submission!\n",
    "We will try our best to keep an engaging commentary running with the code. The code represents our final solution, but with our text blocks we will try to give you some insight into how we reached this.\n",
    "\n",
    "Let's begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- IMPORTS ----------------\n",
    "\n",
    "# Data Handling Tools\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# For clustering of stocks into sectors\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# For making neural net models of indices\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# For data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For random initialization of the trading strategy vector\n",
    "import random\n",
    "\n",
    "# For making waiting fun :)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two blocks help extract data from the .csv files provided, and then extract the returns from them, converting them into basis points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices = []\n",
    "\n",
    "with open(\"data_challenge_stock_prices.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    next(csvreader)\n",
    " \n",
    "    for row in csvreader:\n",
    "        stock_prices.append(row)\n",
    " \n",
    "\n",
    "index_prices = []\n",
    "\n",
    "with open(\"data_challenge_index_prices.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    next(csvreader)\n",
    " \n",
    "    for row in csvreader:\n",
    "        index_prices.append(row)\n",
    "\n",
    "stock_prices = np.array(stock_prices, dtype=float)\n",
    "index_prices = np.array(index_prices, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_returns = [] # Both contain values in Basis Points\n",
    "index_returns = []\n",
    "\n",
    "for i in range(199999):\n",
    "    stock_returns.append(10000 * (stock_prices[i+1] - stock_prices[i]) / stock_prices[i])\n",
    "    index_returns.append(10000 * (index_prices[i+1] - index_prices[i]) / index_prices[i])\n",
    "\n",
    "stock_returns = np.array(stock_returns, dtype=float)\n",
    "index_returns = np.array(index_returns, dtype=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first two problem statements require us to find the partitioning of stocks into sectors with no other data than the returns, our first intuition was to find the correlation between them because:\n",
    "- we lacked any other information linking different stocks\n",
    "- we believed that different stocks of the same sector would exhibit high correlation due to similar market forces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(X, Y): # assumes X and Y same length\n",
    "\n",
    "    n = 0\n",
    "    d1 = 0\n",
    "    d2 = 0\n",
    "    xm = 0\n",
    "    ym = 0\n",
    "    n = X.__len__()\n",
    "\n",
    "    for i in range(n):\n",
    "        xm += X[i]\n",
    "        ym += Y[i]\n",
    "    \n",
    "    xm /= n\n",
    "    ym /= n\n",
    "\n",
    "    for i in range(n):\n",
    "        n += (X[i] - xm)*(Y[i] - ym)\n",
    "        d1 += (X[i] - xm) ** 2\n",
    "        d2 += (Y[i] - ym) ** 2\n",
    "\n",
    "    return (n*1.0)/(math.sqrt(d1*d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.19it/s]\n"
     ]
    }
   ],
   "source": [
    "correl = np.array([[0.1 for _ in range(100)] for _ in range(100)])\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(100):\n",
    "        if i==j:\n",
    "            correl[i][j] = 1\n",
    "            continue\n",
    "        if i>j:\n",
    "            correl[i][j] = correl[j][i]\n",
    "            continue\n",
    "        X = stock_returns[:, i]\n",
    "        Y = stock_returns[:, j]\n",
    "        correl[i][j] = np.corrcoef(X, Y)[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"file_corr.txt\", \"w+\")\n",
    " \n",
    "# Saving the array in a text file\n",
    "for i in range(100):\n",
    "    content = str(correl[i])\n",
    "    file.write(content)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity = 1 - correl\n",
    "hierarchy = linkage(squareform(dissimilarity), method='average')\n",
    "labels = fcluster(hierarchy, 0.88, criterion='distance')\n",
    "for j in range(len(labels)):\n",
    "    labels[j] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [5, 13, 15, 24, 31, 34, 35, 37, 48, 56, 61, 62, 64, 65, 68, 70, 76, 83, 85, 86, 89, 93, 94, 95, 97]\n",
      "1 [2, 3, 6, 7, 9, 16, 17, 19, 23, 28, 32, 33, 42, 47, 51, 53, 58, 60, 63, 66, 72, 81, 87, 91, 98]\n",
      "2 [8, 12, 27, 29, 30, 36, 40, 41, 44, 46, 50, 52, 55, 57, 59, 69, 71, 73, 74, 77, 80, 84, 92, 96, 99]\n",
      "3 [0, 1, 4, 10, 11, 14, 18, 20, 21, 22, 25, 26, 38, 39, 43, 45, 49, 54, 67, 75, 78, 79, 82, 88, 90]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(100):\n",
    "#     print(i, labels[i])\n",
    "\n",
    "clusters = dict()\n",
    "for i in range(100):\n",
    "    if labels[i] not in clusters:\n",
    "        clusters[labels[i]] = [i]\n",
    "    else:\n",
    "        clusters[labels[i]].append(i)\n",
    "\n",
    "\n",
    "for i in range(0, len(clusters)):\n",
    "    print(i, clusters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(1, correl[0][1])\n",
    "# print(2,correl[1][14])\n",
    "# print(3,correl[0][14])\n",
    "# print(4,correl[1][26])\n",
    "# print(5,correl[14][20])\n",
    "# print(6,correl[88][90])\n",
    "# print(7,correl[75][38])\n",
    "# print(8,correl[14][39])\n",
    "# print(9,correl[26][43])\n",
    "# print(10,correl[39][45])\n",
    "# print(11,correl[1][49])\n",
    "# print(12,correl[38][75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_rel(x, y):\n",
    "    re = []\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            if labels[i] == x and labels[j] == y:\n",
    "                re.append(correl[i][j])\n",
    "\n",
    "    return re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.608\n",
      "1 3.79\n",
      "2 4.06\n",
      "3 19.534\n"
     ]
    }
   ],
   "source": [
    "for p in range(4):\n",
    "    a = cross_rel(3, p)\n",
    "    print(p, round(100*np.mean(a), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_group_correl = [[0.0 for _ in range(15)] for _ in range(100)]\n",
    "for i in range(100):\n",
    "    for j in range(15):\n",
    "        X = stock_returns[:, i]\n",
    "        Y = index_returns[:, j]\n",
    "        stock_group_correl[i][j] = np.corrcoef(X,Y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_index_analyze(grp, ind):\n",
    "    for stck in clusters[grp]:\n",
    "        print(stck, stock_group_correl[stck][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 -0.043444624028860825\n",
      "13 -0.054021430920410674\n",
      "15 -0.11968848181583566\n",
      "24 -0.055484313088256194\n",
      "31 -0.04956152382931762\n",
      "34 -0.04170376823782463\n",
      "35 -0.054467331363896546\n",
      "37 -0.05227058212103044\n",
      "48 -0.05637425827865387\n",
      "56 -0.058204794919400775\n",
      "61 -0.06425740896053413\n",
      "62 -0.06291187068245913\n",
      "64 -0.06253907102175497\n",
      "65 -0.0643838233217589\n",
      "68 -0.06990123288624457\n",
      "70 -0.06496109876331742\n",
      "76 -0.06806300599969749\n",
      "83 -0.06934472949092621\n",
      "85 -0.08595294220833403\n",
      "86 -0.18991076019572092\n",
      "89 -0.08645366951399902\n",
      "93 -0.08655172741848911\n",
      "94 -0.14035460483535137\n",
      "95 -0.09149518993715464\n",
      "97 -0.10238044631424775\n"
     ]
    }
   ],
   "source": [
    "group_index_analyze(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[[0 for _ in range(25)] for _ in range(199999)] for _ in range(4)]\n",
    "\n",
    "p=0\n",
    "for i in clusters[0]:\n",
    "    \n",
    "    for j in range(199999):\n",
    "        data[0][j][p] = stock_returns[j][i]\n",
    "    \n",
    "    p +=1\n",
    "\n",
    "p=0\n",
    "for i in clusters[1]:\n",
    "    \n",
    "    for j in range(199999):\n",
    "        data[1][j][p] = stock_returns[j][i]\n",
    "    \n",
    "    p +=1\n",
    "p=0\n",
    "for i in clusters[2]:\n",
    "    \n",
    "    for j in range(199999):\n",
    "        data[2][j][p] = stock_returns[j][i]\n",
    "    \n",
    "    p +=1\n",
    "\n",
    "p=0\n",
    "\n",
    "for i in clusters[3]:\n",
    "    for j in range(199999):\n",
    "        data[3][j][p] = stock_returns[j][i]\n",
    "    \n",
    "    p +=1\n",
    "\n",
    "\n",
    "# data4 = torch.FloatTensor(data3)\n",
    "\n",
    "# batches = 200\n",
    "\n",
    "# model = nn.Sequential(nn.Linear(25,64),nn.ReLU(),nn.Linear(64,64),nn.ReLU(),nn.Linear(64,1))\n",
    "# facts = nn.L1Loss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# for e in tqdm(range(500)):\n",
    "#     for k in range(batches):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data4[int(k*199999/batches):int((k+1)*199999/batches), :])\n",
    "#         loss = facts(output, torch.unsqueeze(torch.FloatTensor(index_returns[int(k*199999/batches):int((k+1)*199999/batches),0]), 1))\n",
    "#         if ((k == 0 and e==0) or k == batches-1) and e%100 == 0:\n",
    "#             print(loss.item())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_index(index, data, epochs=500): # data contains a 199999x25 array containing returns of a sector\n",
    "\n",
    "    batches = 200\n",
    "    model = nn.Sequential(nn.Linear(25,64),nn.ReLU(),nn.Linear(64,256),nn.ReLU(),nn.Linear(256,64),nn.ReLU(),nn.Linear(64,1))\n",
    "    facts = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    data4 = torch.FloatTensor(data)\n",
    "\n",
    "\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        for k in range(batches):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data4[int(k*data.__len__()/batches):int((k+1)*data.__len__()/batches), :])\n",
    "            loss = facts(output, torch.unsqueeze(torch.FloatTensor(index_returns[int(k*data.__len__()/batches):int((k+1)*data.__len__()/batches),index]), 1))\n",
    "\n",
    "            l2_lambda = 0.0008\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    " \n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model0_3 = make_model_for_index(9, data[2], epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = np.array(torch.squeeze(model0_3(torch.FloatTensor(data[2])).detach()))\n",
    "# target = np.array(index_returns[:, 9])\n",
    "# print(np.mean(np.abs(output-target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:09<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 44.61740303456987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 40.304720120482266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 55.241561204232404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 43.05585613912948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 44.54591889977516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 53.73713259581184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 44.652635607516714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 40.07831938091497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 40.36811669249113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 43.68263309504979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 43.33222396685063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 43.788018565621826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 43.59207690891128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 44.14143524136973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 53.644559431567885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_to_group = [3,0,3,1,2,0,1,3,1,2,0,3,2,1,2] # TODO: Fill\n",
    "index_models = [] # will contain models trained on first 170k samples, will verify on rest 30k\n",
    "pred_correl = []\n",
    "\n",
    "for i in range(15):\n",
    "    index_models.append(make_model_for_index(i, data[index_to_group[i]][:170000][:], epochs=30))\n",
    "    predictions = np.array(torch.squeeze(index_models[-1](torch.FloatTensor(data[index_to_group[i]][170000:][:])).detach()))\n",
    "    pred_correl.append(np.corrcoef(predictions,[index_returns[p][i] for p in range(170000, 199999)])[0,1])\n",
    "    print(i, 100*pred_correl[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n"
     ]
    }
   ],
   "source": [
    "mu = []\n",
    "for i in range(15):\n",
    "    mu.append(np.array(torch.squeeze(index_models[-1](torch.FloatTensor(data[index_to_group[i]][:][:])).detach())))\n",
    "\n",
    "mu = np.array(mu)\n",
    "\n",
    "covar = np.cov(mu)\n",
    "print(covar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_alpha(A): # A is the 1-d array of allocations, we will use this function to satisfy 2nd and 3rd bullet points of 5th problem statement\n",
    "    A = np.array(A)\n",
    "    mn = A.mean()\n",
    "    A = A - np.array([mn for _ in range(len(A))])\n",
    "    mx = max(abs(A))\n",
    "    for i in range(len(A)):\n",
    "        A[i] /= mx\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpha(mu_t, covv): # takes Mu_t and covariance matrix and returns A (allocation)\n",
    "    \n",
    "    \"\"\"-----------MAKE STRATEGY HERE---------------\n",
    "    and preferably explain it\"\"\"\n",
    "    cst = 100\n",
    "    cst_rt = 10\n",
    "    old_cst = 200\n",
    "    gamma = 0.13\n",
    "    lr = 0.002\n",
    "\n",
    "    A = np.array([random.random() for _ in range(15)])\n",
    "\n",
    "    while( abs(cst - old_cst) > 0.001 ):\n",
    "        A = normalize_alpha(A)\n",
    "        old_cst = cst\n",
    "        cst = np.dot(A,mu_t) - gamma * np.sum([[A[i]*A[j]*covv[i][j] for i in range(15)] for j in range(15)])\n",
    "        \n",
    "        grad = np.array([mu_t[g] - gamma * 2 * np.sum([covv[g][h] * A[h] for h in range(15)]) for g in range(15)])\n",
    "\n",
    "        A = A + lr * grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"================ REMEMBER TO NORMALIZE====================\"\"\"\n",
    "\n",
    "    A = normalize_alpha(A)\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Alpha(Alph):\n",
    "    day_ret = []\n",
    "\n",
    "    for t in tqdm(range(191550,191599)):\n",
    "\n",
    "        A = Alpha([mu[p][t] for p in range(15)], covar)\n",
    "        actual_returns = [index_returns[t][p] for p in range(15)]\n",
    "        day_ret.append(np.dot(A,actual_returns))\n",
    "\n",
    "    day_ret = np.array(day_ret)\n",
    "\n",
    "    print(f\"Mean daily returns: {day_ret.mean()}\")\n",
    "    print(f\"Standard deviation: {day_ret.std()}\")\n",
    "    print(f\"Sharpe ratio: {day_ret.mean()/day_ret.std()}\") \n",
    "    # Since long and short positions net out, the risk-free return rate is 0 (not buying nor selling).\n",
    "    # Hence, in Sharpe formula, Ra - Rb = Ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:07<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean daily returns: 1.920291763029162\n",
      "Standard deviation: 4.401499645481538\n",
      "Sharpe ratio: 0.43628124905121424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_Alpha(Alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
